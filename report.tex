\documentclass[conference,a4paper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Command to include a center figure
\newcommand{\centerfigure}[2]{
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\linewidth]{figures/#1.png}
        \caption{#2}
        \label{fig:#1}
    \end{figure}
}

\newcommand{\centertable}[2]{
    \begin{table}[htbp]
        \centering
        \caption{#2}
        \input{figures/#1.tex}
        \label{tab:#1}
    \end{table}
}

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\begin{document}

\title{AIML 425 Assignment 4 %\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{James Thompson}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{Victoria University of Wellington}\\
%City, Country \\
300680096}

% Figure example import part is the htbp
% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

\maketitle

\section{Introduction}

This document reports the implementation and results of Assignment 4. I look at the differences between the Stochastic Differential Equations (SDE) and Ordinary Differential Equation models (ODE). The SDE model is a score based generative model to learn a mapping from Gaussian noise to dog images. The ODE model is a normalizing flow model that learns a mapping from Gaussian to dogs and cats to dogs. For simplicity I restrict the images to be simple 2D (two pixel) images.

\section{Theory}

A flow is a continuous transformation from one distribution to another through a process of small changes. In generative modeling, flows provide a powerful framework for learning how to transport samples from a source distribution (e.g Gaussian noise) to a complex target distribution (cat images) over time $t \in [0, 1]$. The key difference between an SDE and ODE is that one is stochastic and the other deterministic.

We are interested in the transformations of the distribution of probability mass. A single particle is modeled with a stochastic differential equation:
\begin{equation}
    \label{eq:sde_particle}
    dx = a(x, t)dt + b(x, t)dW
\end{equation}
where $x$ is the state of the particle, $a(x, t)$ is the drift coefficient (a vector field), $b(x, t)$ is the diffusion coefficient (a scalar), and $dW$ is a Wiener process increment (Gaussian noise). The first term represents deterministic drift, while the second term accounts for stochastic diffusion.

When we have many particles moving according to this SDE, the distribution of their positions evolves over time. This is known as the probability flux and can be described with the Fokker-Planck equation.
\begin{equation}
    \label{eq:fokker-planck}
    \frac{\partial p(x, t)}{\partial t} = -\nabla_x \cdot (a(x, t)p(x, t)) + \frac{1}{2}\nabla^2_x(b^2(x, t)p(x, t))
\end{equation}

where $p(x, t)$ is the probability density at point $x$ and time $t$ and the rest is the same as the SDE \ref{eq:sde_particle} above.

\subsection{Stochastic Differential Equations}

Equilibrium is reached when the probability density no longer changes with time, i.e. $\frac{\partial p(x, t)}{\partial t} = 0$. This happens when the random diffusion balances the deterministic drift. In the context of SDEs, this looks like:
\begin{equation}
    \label{eq:equilibrium}
    0 = -kT \nabla_x \cdot (p(x, t) \nabla_x \log p(x,t)) + kT \nabla_x^2 p(x, t)
\end{equation}
where $k$ is the Boltzmann constant and $T$ is the temperature (a tunable hyperparameter). This equation describes how particles in a fluid reach thermal equilibrium through a balance of drift and diffusion. From this we can see the score function $\nabla_x \log p(x,t)$ plays a crucial role in determining the drift that counteracts diffusion.

A Stochastic Differential Equation (SDE) generative model \cite{songScoreBasedGenerativeModeling2021} is a continuous-time generative model that learns the score function of a data distribution. 
\begin{equation}
s_\theta(x, t) \approx \nabla_x \log p(x,t)
\end{equation}
where $\theta$ are the parameters of the neural network.

% maybe move this to the experiment section
Creating the scores is done using a variance-preserving noise schedule where clean target data $y$ is corrupted with Gaussian noise $\epsilon$ at time $t$:

\begin{equation}
    x_t = \sqrt{\alpha_t}y + \sigma_t\epsilon, \quad \text{where } \alpha_t = (1-t)^2, \; \sigma_t = \sqrt{1 - \alpha_t}
\end{equation}
The noise schedule is designed such that with $t \in [0, 1]$ the variance will stay constant which allows numerical stability. The neural network learns to predict the score function $s_\theta(x_t, t)$ directly. The training target is:
\begin{equation}
    s_{\text{true}} = \frac{\sqrt{\alpha_t}y - x_t}{1 - \alpha_t}
\end{equation}
We can then use mean squared error between predicted score and true score to train the network. This "predict the difference" approach is more effective than predicting the clean image directly, as the network learns to focus on the correction needed rather than reconstructing the entire signal.

During generation, I integrate backward in time from $t=1$ to $t=0$ using the Euler-Maruyama method \cite{burdenNumericalAnalysis2016}:
\begin{equation}
    x_{t-dt} = x_t - \frac{\beta_t}{2}(x_t + 2s_\theta(x_t, t))dt + \sqrt{\beta_t dt}\epsilon
\end{equation}
where $\beta_t = 2/(1-t)$ is the noise schedule parameter. This process reverses the forward diffusion, guided by the learned score.

\subsection{Ordinary Differential Equations}

An alternative method to SDEs is to use an Ordinary Differential Equation (ODE) model. A Flow matching ODE learns a velocity field $v_\theta(x, t)$ that defines a continuous normalizing flow:
\begin{equation}
    \frac{dx}{dt} = v_\theta(x, t)
\end{equation}

Given distributions $p_0(x)$ (source) and $p_1(x)$ (target), we define a target velocity field through interpolation $I_t(x_0, x_1, t)$:
\begin{equation}
    x_t = I_t(x_0, x_1, t), \quad v_{\text{target}}(x, t) = \mathbb{E}_{X_0, X_1}[\dot{I}(X_0, X_1, t) | X_t = x]
\end{equation}
where $\dot{I}$ is the time derivative of the interpolation, which is the velocity and is constant magnitude between $t=0$ and $t=1$. 

Computing $v_{\text{target}}(x, t)$ requires evaluating the conditional expectation $\mathbb{E}_{X_0, X_1}[\dot{I}(X_0, X_1, t) | X_t = x]$, which is intractable in practice. We would need to know which pairs $(x_0, x_1)$ produce a given $x_t$ and their probabilities, which requires knowing the full joint distribution. In practice, conditional flow matching is used with linear interpolation.
\begin{equation}
    x_t = (1-t)x_0 + tx_1, \quad \dot{I}(x_0, x_1, t) = x_1 - x_0
\end{equation}

\noindent This gives the practical training loss, which is equivalent to the mean squared error between the predicted and true velocity:
\begin{equation}
    \mathcal{L} = \mathbb{E}_t \mathbb{E}_{X_0, X_1} \|v_\theta(X_t, t) - (X_1 - X_0)\|^2
\end{equation}

The neural network learns to predict the velocity $v(x, t) = x_1 - x_0$ directly, which is simpler than learning score functions. During generation, we integrate forward in time from $t=0$ to $t=1$ using the Euler method \cite{burdenNumericalAnalysis2016}:
\begin{equation}
    x_{t+dt} = x_t + v_\theta(x_t, t) \cdot dt
\end{equation}

This flow matching approach is computationally simpler than SDEs, it doesn't require sampling random noise at each step, making generation deterministic and often faster. However, the stochastic sampling in SDEs can help "heal" small errors in the learned score function, potentially improving robustness.

\subsection{Performance comparison} 

The end objective of both models is to learn how to take data from one distribution to another iteratively. The loss shows us the difference between the flows. However to understand the performance we really want to know how well it can generate samples. A simple and effective measure is Maximum Mean Discrepancy (MMD) \cite{grettonKernelMethodTwoSample2008} which allows us to compare the generated samples to the real target distribution. The MMD is a measure of the distance between two distributions based on samples from each distribution. It is defined as

\begin{align}
    \text{MMD}^2(P, Q) &= \mathbb{E}[k(x, x')] + \mathbb{E}[k(y, y')] - 2\mathbb{E}[k(x, y)]\\
    \text{where } k(x, y) &= \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right)\\
    \text{and } &x, x' \sim P, \quad y, y' \sim Q 
\end{align}

The only parameter in this is the kernel bandwidth $\sigma$. The higher the sigma the smoother the kernel so focuses on global differences, where lower sigma focuses on local differences.

Other metrics to understand the performance can be learning efficiency and sample efficiency. This means that we can compare how many samples it takes to learn as well as how much compute time it takes to learn. This can be measured in terms of number of epochs or wall clock time.

\section{Experiments}

\subsection{Data}
The data is generated by sampling from the respective distributions. The SDE models train with 100,000 examples while the ODE models train with only 10,000 examples. The Gaussian is with scale 1, Dogs are images in the uniform square with corners (-1,1) and (-2,2) and Cats are images in the uniform square with corners (2,-2) and (3,-3). At dataset creation time the target is computed using either a interpolation technique (flow matching) or a noise corruption technique (score matching). In both methods the Gaussian noise had a scale of 1. We can see examples of the data in Figure \ref{fig:individual-data-samples}.

\subsection{Models}
The model architecture for both the SDE and ODE is similar. They are all fully connected feed forward networks with 3-4 hidden layers and ReLU activations. The SDE model is larger due to the less efficient training. Complete hyperparameters are found in the appendix Table \ref{tab:hyp}.

\subsubsection{SDE}
The SDE model is trained to learn the score function which is used to reverse the diffusion process. A visualization of the data can be found in Figure \ref{fig:noise-process-dogs}. The final performance is satisfactory as judged visually in Figure \ref{fig:sde-generation}. Furthermore comparing the score field in Figure \ref{fig:sde-score-field} with the true score field in Figure \ref{fig:noise-process-dogs} shows that the model has learned a good approximation of the score function.

\subsubsection{ODE}
The ODE model is trained to learn the velocity field which models the flow of data from one distribution to another. To understand the training data we can visualize the interpolation process in Figure \ref{fig:interpolation-gaussians-to-dogs} for the Gaussian to Dogs  and \ref{fig:interpolation-cats-to-dogs} for Cats to Dogs. For both of the mappings the performance is sufficiently good as can be seen in sample generations in Figures \ref{fig:ode-gaussdog-generation} and \ref{fig:ode-catdog-generation}. The velocity fields learned by the models are good approximation of the true velocity fields as seen in Figures \ref{fig:ode-gaussdog-velocity-field} and \ref{fig:ode-catdog-velocity-field}. Interestingly we can see in the Cat to Dog mapping the score field always points up to the left as that is all the model needs to learn (i.e take point in and move it along the left to right diagonal). This works because the $t$ is bounded to [0, 1] so the model simply learns the direction of travel not the distance it needs to travel. This is noticeably more complex than the Gaussian to Dog case where it need to learn the direction and distance.

\subsection{SDE vs ODE}

As discussed above we can quantify the performance of the models using MMD. This works by taking samples from the target distribution and comparing them to samples generated by the model. As MMD is a distance metric the lower the better and we can see that the SDE actually performs slightly better than the ODE model for Gaussian to Dog generation in Table \ref{tab:mmd}. It is worth noting however that the ODE model was trained using much less data, less training epochs and smaller model size. When the SDE was trained with similar parameters as the ODE it performance was unsatisfactory. This is due to the fact that the ODE is trained on actual example trajectory which is much faster to learn from then simply noisy image samples.
\section{Conclusion}

In this assignment I implemented and trained both SDE and ODE generative models. Both models were able to learn the mappings they were trained on, with the SDE model performing slightly better in terms of MMD. However the ODE models were much more efficient to train, requiring less data, smaller models and fewer epochs. This highlights the tradeoff between model complexity and training efficiency in generative modeling.


\newpage
\section*{Statement}

The code and report of the Assignment was solely completed by myself (Thompson James). The complete source code can be found here \url{https://gitea.james-server.duckdns.org/james/AIML425\_assignment\_4}, with a link to a colab notebook found here: \url{https://colab.research.google.com/github/1jamesthompson1/AIML425_assignment_4/blob/main/main.ipynb}. A complete run through of the notebook takes about 10 minutes on a GPU enabled machine.

% I have kept the appendix as concise as possible, however the code is setup to produce many more figures and tables that can be used to understand the models. Most of these figures have been generated and stored in the `figures` folder.

I completed my work using the following tools:
\begin{itemize}
    \item \textbf{Jupyter Notebook \cite{Kluyver2016jupyter} and JupyterText \cite{woutsMwoutsJupytext2025}:} For interactive development.
    \item \textbf{\LaTeX}: For writing the report.
    \item \textbf{VSCode \cite{MicrosoftVscode2025}:} As IDE, with Copilot to help with plotting and debugging.
    \item \textbf{JAX \cite{jax2018github} and Flax \cite{flax2020github}:} For implementing the neural network and training logic.
    \item \textbf{Matplotlib\cite{Hunter:2007} and Pandas\cite{thepandasdevelopmentteamPandasdevPandasPandas}:} For data visualization and management.
\end{itemize}


\bibliographystyle{IEEEtran}
\bibliography{references}

\appendix

\section{Model experiments}


\subsection{Setup}
\begin{table}
    \caption{Hyperparameters used for each model}
    \label{tab:hyp}
    \centering
    \begin{tabular}{lccc}
        \toprule
        Hyperparameter & SDE & ODE (Gauss to Dog) & ODE (Cat to Dog) \\
        \midrule
        Learning Rate & \multicolumn{3}{c}{0.0001} \\
        Minibatch Size & 512 & \multicolumn{2}{c}{256} \\
        Hidden Dims & 512 & \multicolumn{2}{c}{256}\\
        Hidden Layers & 4 & \multicolumn{2}{c}{3} \\
        Num Epochs & 500 & \multicolumn{2}{c}{200}\\
        Optimizer & \multicolumn{3}{c}{Adam} \\
        Loss Function & \multicolumn{3}{c}{MSE} \\
        Generation dt & 0.0001 & \multicolumn{2}{c}{0.001} \\
        \midrule
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Results}
The models were trained using the hyperparameters in Table \ref{tab:hyp}. They were trained on a RTX A5000 GPU with a Xeon Gold 6128 CPU. 
\begin{table}
    \caption{Training results}
    \label{tab:mmd}
    \centering
    \begin{tabular}{lccc}
        \toprule
        Result & SDE & ODE (Gauss to Dog) & ODE (Cat to Dog) \\
        Runtime (seconds) & 273 & 50 & 24 \\
        MMD & 0.000866 & 0.001344 & 0.000560 \\
        Best validation loss & 1.7442 & 0.4384 & 0.1322 \\
        \midrule
        \bottomrule
    \end{tabular}
\end{table}

\section{Figures}

\centerfigure{individual-data-samples}{Samples from the datasets used for training. The values are normalized to [0, 1] and plotted as images to understand the data.}

\subsection{SDE: Gaussian to Dogs}

\centerfigure{noise-process-dogs}{The forward noise process corrupting dog images over time.}
\centerfigure{sde-generation}{Sample generations of Dogs from Gaussian noise using the SDE model.}
\centerfigure{sde-score-field}{The score field learned by the SDE model compared to the true score field.}

\subsection{ODE: Gaussian to Dogs}

\centerfigure{interpolation-gaussians-to-dogs}{Linear interpolation between Gaussian noise and Dog images used for training the ODE model.}
\centerfigure{ode-gaussdog-generation}{Sample generations of Dogs from Gaussian noise using the ODE model.}
\centerfigure{ode-gaussdog-velocity-field}{The velocity field learned by the ODE model compared to the true velocity field.}
\subsection{ODE: Cats to Dogs}

\centerfigure{interpolation-cats-to-dogs}{Linear interpolation between Cat images and Dog images used for training the ODE model.}
\centerfigure{ode-catdog-generation}{Sample generations of Dogs from Cat images using the ODE model.}
\centerfigure{ode-catdog-velocity-field}{The velocity field learned by the ODE model compared to the true velocity field.}



\end{document}
